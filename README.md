# Kaggle-Learn

[![Kaggle](https://img.shields.io/badge/Kaggle-Learn-blue)](https://www.kaggle.com/learn)
[![Made with Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Kaggle íŠœí† ë¦¬ì–¼ì—ì„œ ê³µë¶€í•œ ë‚´ìš©ê³¼ ì‹¤ìŠµ ë…¸íŠ¸ë¶ì„ ê¸°ë¡, ì •ë¦¬í•˜ëŠ” ë ˆí¬ì§€í† ë¦¬ì…ë‹ˆë‹¤.

---

## ğŸ [Tutorial 1 - Python](https://www.kaggle.com/learn/python)  
â³ í•™ìŠµ ê¸°ê°„: **25.06.24 ~ 25.07.13**

### Lecture Notes
- [x] 01 - í—¬ë¡œ íŒŒì´ì¬ ([Hello Python](https://www.kaggle.com/code/colinmorris/hello-python))
- [x] 02 - í•¨ìˆ˜ì™€ ë„ì›€ë§ ([Functions and Getting Help](https://www.kaggle.com/code/colinmorris/functions-and-getting-help))
- [x] 03 - ë¶ˆë¦¬ì–¸ê³¼ ì¡°ê±´ë¬¸ ([Booleans and Conditionals](https://www.kaggle.com/code/colinmorris/booleans-and-conditionals))
- [x] 04 - ë¦¬ìŠ¤íŠ¸ ([Lists](https://www.kaggle.com/code/colinmorris/lists))
- [x] 05 - ë°˜ë³µë¬¸ê³¼ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ([Loops and List Comprehensions](https://www.kaggle.com/code/colinmorris/loops-and-list-comprehensions))
- [x] 06 - ë¬¸ìì—´ê³¼ ë”•ì…”ë„ˆë¦¬ ([Strings and Dictionaries](https://www.kaggle.com/code/colinmorris/strings-and-dictionaries))

### Exercises
- [x] 01 - ë¬¸ë²•, ë³€ìˆ˜, ìˆ«ì ([Syntax Variables and Numbers](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.06.24%20Hello%2C%20Python/exercise-syntax-variables-and-numbers.ipynb))  
- [x] 02 - í•¨ìˆ˜ì™€ ë„ì›€ë§ ([Functions and Getting Help](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.06.28%20Functions%20and%20Getting%20Help/exercise-functions-and-getting-help.ipynb))  
- [x] 03 - ë¶ˆë¦¬ì–¸ê³¼ ì¡°ê±´ë¬¸ ([Booleans and Conditionals](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.07%20Booleans%20and%20Conditionals/exercise-booleans-and-conditionals.ipynb))  
- [x] 04 - ë¦¬ìŠ¤íŠ¸ ([Lists](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.07%20Lists/exercise-lists.ipynb))  
- [x] 05 - ë°˜ë³µë¬¸ê³¼ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ([Loops and List Comprehensions](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.10%20Loops%20and%20List%20Comprehensions/exercise-loops-and-list-comprehensions.ipynb))  
- [x] 06 - ë¬¸ìì—´ê³¼ ë”•ì…”ë„ˆë¦¬ ([Strings and Dictionaries](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.11%20Strings%20and%20Dictionaries/exercise-strings-and-dictionaries.ipynb))  
- [x] 07 - ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© ([Working with external Libraries](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.13%20Working%20with%20External%20Libraries/exercise-working-with-external-libraries.ipynb))

---

## ğŸ¼ [Tutorial 2 - Pandas](https://www.kaggle.com/learn/pandas)  
â³ í•™ìŠµ ê¸°ê°„: **25.06.29 ~ 25.07.17**

### Lecture Notes
- [x] 01 - ìƒì„±, ì½ê¸°, ì“°ê¸° ([Creating, Reading, and Writing](https://www.kaggle.com/code/residentmario/creating-reading-and-writing))  
- [x] 02 - ì¸ë±ì‹±, ì„ íƒ, í• ë‹¹ ([Indexing, Selecting, and Assigning](https://www.kaggle.com/code/residentmario/indexing-selecting-assigning))  
- [x] 03 - ìš”ì•½ í•¨ìˆ˜ì™€ ë§¤í•‘ ([Summary Functions and Maps](https://www.kaggle.com/code/residentmario/summary-functions-and-maps))  
- [x] 04 - ê·¸ë£¹í™”ì™€ ì •ë ¬ ([Grouping and Sorting](https://www.kaggle.com/code/residentmario/grouping-and-sorting))  
- [x] 05 - ë°ì´í„° íƒ€ì…ê³¼ ê²°ì¸¡ì¹˜ ([Data Types and Missing Values](https://www.kaggle.com/code/residentmario/data-types-and-missing-values))  
- [x] 06 - ì´ë¦„ ë³€ê²½ê³¼ ê²°í•© ([Renaming and Combining](https://www.kaggle.com/code/residentmario/renaming-and-combining))  

### Exercises
- [x] 01 - ìƒì„±, ì½ê¸°, ì“°ê¸° ([Creating, Reading, and Writing]())  
- [x] 02 - ì¸ë±ì‹±, ì„ íƒ, í• ë‹¹ ([Indexing, Selecting, and Assigning]())  
- [x] 03 - ìš”ì•½ í•¨ìˆ˜ì™€ ë§¤í•‘ ([Summary Functions and Maps]())  
- [x] 04 - ê·¸ë£¹í™”ì™€ ì •ë ¬ ([Grouping and Sorting]())  
- [x] 05 - ë°ì´í„° íƒ€ì…ê³¼ ê²°ì¸¡ì¹˜ ([Data Types and Missing Values]())  
- [x] 06 - ì´ë¦„ ë³€ê²½ê³¼ ê²°í•© ([Renaming and Combining]())  

---

## ğŸ§¹ [Tutorial 3 - Data Cleaning](https://www.kaggle.com/learn/data-cleaning)  
â³ í•™ìŠµ ê¸°ê°„: **25.07.02 ~ 25.07.25**

### Lecture Notes
- [x] 01 - ê²°ì¸¡ì¹˜ ë‹¤ë£¨ê¸° ([Handling Missing Values](https://www.kaggle.com/code/alexisbcook/handling-missing-values))  
- [x] 02 - ìŠ¤ì¼€ì¼ë§ê³¼ ì •ê·œí™” ([Scaling and Normalization](https://www.kaggle.com/code/alexisbcook/scaling-and-normalization))  
- [x] 03 - ë‚ ì§œ íŒŒì‹± ([Parsing Dates](https://www.kaggle.com/code/alexisbcook/parsing-dates))  
- [x] 04 - ë¬¸ì ì¸ì½”ë”© ([Character Encodings](https://www.kaggle.com/code/alexisbcook/character-encodings))  
- [x] 05 - ë¶ˆì¼ì¹˜ ë°ì´í„° ì…ë ¥ ([Inconsistent Data Entry](https://www.kaggle.com/code/alexisbcook/inconsistent-data-entry))  

### Exercises
- [x] 01 - ê²°ì¸¡ì¹˜ ë‹¤ë£¨ê¸° ([Handling Missing Values]())  
- [x] 02 - ìŠ¤ì¼€ì¼ë§ê³¼ ì •ê·œí™” ([Scaling and Normalization]())  
- [x] 03 - ë‚ ì§œ íŒŒì‹± ([Parsing Dates]())  
- [x] 04 - ë¬¸ì ì¸ì½”ë”© ([Character Encodings]())  
- [x] 05 - ë¶ˆì¼ì¹˜ ë°ì´í„° ì…ë ¥ ([Inconsistent Data Entry]())  

---

## ğŸ¤– [Tutorial 4 - Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)  
â³ í•™ìŠµ ê¸°ê°„: **25.07.06 ~ 25.08.08**

### Lecture Notes
- [x] 01 - ëª¨ë¸ì˜ ì‘ë™ ì›ë¦¬ ([How Models Work](https://www.kaggle.com/code/dansbecker/how-models-work))  
- [x] 02 - ê¸°ë³¸ ë°ì´í„° íƒìƒ‰ ([Basic Data Exploration](https://www.kaggle.com/code/dansbecker/basic-data-exploration))  
- [x] 03 - ì²« ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ([Your First Machine Learning Model](https://www.kaggle.com/code/dansbecker/your-first-machine-learning-model))  
- [x] 04 - ëª¨ë¸ ê²€ì¦ ([Model Validation](https://www.kaggle.com/code/dansbecker/model-validation))  
- [x] 05 - ê³¼ì†Œì í•©ê³¼ ê³¼ì í•© ([Underfitting and Overfitting](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting))  
- [x] 06 - ëœë¤ í¬ë ˆìŠ¤íŠ¸ ([Random Forests](https://www.kaggle.com/code/dansbecker/random-forests))  
- [x] 07 - ë¨¸ì‹ ëŸ¬ë‹ ëŒ€íšŒ ([Machine Learning Competitions](https://www.kaggle.com/code/dansbecker/machine-learning-competitions))  

### Exercises
- [x] 02 - ë°ì´í„° íƒìƒ‰ ([Explore your data]())  
- [x] 03 - ì²« ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ([Your First Machine Learning Model]())  
- [x] 04 - ëª¨ë¸ ê²€ì¦ ([Model Validation]())  
- [x] 05 - ê³¼ì†Œì í•©ê³¼ ê³¼ì í•© ([Underfitting and Overfitting]())  
- [x] 06 - ëœë¤ í¬ë ˆìŠ¤íŠ¸ ([Random Forests]())  
- [x] 07 - ë¨¸ì‹ ëŸ¬ë‹ ëŒ€íšŒ ([Machine Learning Competitions]())  

---

## ğŸ“ˆ [Tutorial 5 - Data Visualization](https://www.kaggle.com/learn/data-visualization)  
â³ í•™ìŠµ ê¸°ê°„: **25.07.01 ~ 25.08.16**

### Lecture Notes
- [x] 01 - í—¬ë¡œ ì‹œë³¸ ([Hello Seaborn](https://www.kaggle.com/code/alexisbcook/hello-seaborn))  
- [x] 02 - ì„  ê·¸ë˜í”„ ([Line Charts](https://www.kaggle.com/code/alexisbcook/line-charts))  
- [x] 03 - ë§‰ëŒ€ ê·¸ë˜í”„ì™€ íˆíŠ¸ë§µ ([Bar Charts and Heatmaps](https://www.kaggle.com/code/alexisbcook/bar-charts-and-heatmaps))  
- [x] 04 - ì‚°ì ë„ ([Scatter Plots](https://www.kaggle.com/code/alexisbcook/scatter-plots))  
- [x] 05 - ë¶„í¬ ([Distributions](https://www.kaggle.com/code/alexisbcook/distributions))  
- [x] 06 - ê·¸ë˜í”„ ì„ íƒê³¼ ìŠ¤íƒ€ì¼ ([Choosing Plot Types and Custom Styles](https://www.kaggle.com/code/alexisbcook/choosing-plot-types-and-custom-styles))  
- [x] 07 - ìµœì¢… í”„ë¡œì íŠ¸ ([Final Project](https://www.kaggle.com/code/alexisbcook/final-project))  
- [x] 08 - ë‚˜ë§Œì˜ ë…¸íŠ¸ë¶ ë§Œë“¤ê¸° ([Creating Your Own Notebook](https://www.kaggle.com/code/alexisbcook/creating-your-own-notebook))  

### Exercises
- [x] 01 - í—¬ë¡œ ì‹œë³¸ ([Hello Seaborn]())  
- [x] 02 - ì„  ê·¸ë˜í”„ ([Line Charts]())  
- [x] 03 - ë§‰ëŒ€ ê·¸ë˜í”„ì™€ íˆíŠ¸ë§µ ([Bar Charts and Heatmaps]())  
- [x] 04 - ì‚°ì ë„ ([Scatter Plots]())  
- [x] 05 - ë¶„í¬ ([Distributions]())  
- [x] 06 - ê·¸ë˜í”„ ì„ íƒê³¼ ìŠ¤íƒ€ì¼ ([Choosing Plot Types and Custom Styles]())  
- [x] 07 - ìµœì¢… í”„ë¡œì íŠ¸ ([Final Project]())  

---

## ğŸ“Š [Tutorial 6 - Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)  
â³ í•™ìŠµ ê¸°ê°„: 

### Lecture Notes
- [ ] 01 - ì†Œê°œ ([Introduction](https://www.kaggle.com/code/alexisbcook/introduction))  
- [ ] 02 - ê²°ì¸¡ì¹˜ ([Missing Values](https://www.kaggle.com/code/alexisbcook/missing-values))  
- [ ] 03 - ë²”ì£¼í˜• ë³€ìˆ˜ ([Categorical Variables](https://www.kaggle.com/code/alexisbcook/categorical-variables))  
- [ ] 04 - íŒŒì´í”„ë¼ì¸ ([Pipelines](https://www.kaggle.com/code/alexisbcook/pipelines))  
- [ ] 05 - êµì°¨ ê²€ì¦ ([Cross-Validation](https://www.kaggle.com/code/alexisbcook/cross-validation))  
- [ ] 06 - XGBoost ([XGBoost](https://www.kaggle.com/code/alexisbcook/xgboost))  
- [ ] 07 - ë°ì´í„° ëˆ„ìˆ˜ ([Data Leakage](https://www.kaggle.com/code/alexisbcook/data-leakage))  

### Exercises
- [ ] 01 - ì†Œê°œ ([Introduction]())  
- [ ] 02 - ê²°ì¸¡ì¹˜ ([Missing Values]())  
- [ ] 03 - ë²”ì£¼í˜• ë³€ìˆ˜ ([Categorical Variables]())  
- [ ] 04 - íŒŒì´í”„ë¼ì¸ ([Pipelines]())  
- [ ] 05 - êµì°¨ ê²€ì¦ ([Cross-Validation]())  
- [ ] 06 - XGBoost ([XGBoost]())  
- [ ] 07 - ë°ì´í„° ëˆ„ìˆ˜ ([Data Leakage]())

---

## ğŸ§  [Tutorial 7 - Introduction to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)  
â³ í•™ìŠµ ê¸°ê°„: 

### Lecture Notes
- [ ] 01 - ë‹¨ì¼ ë‰´ëŸ° ([A Single Neuron](https://www.kaggle.com/code/dansbecker/a-single-neuron))  
- [ ] 02 - ì‹¬ì¸µ ì‹ ê²½ë§ ([Deep Neural Networks](https://www.kaggle.com/code/dansbecker/deep-neural-networks))  
- [ ] 03 - í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• ([Stochastic Gradient Descent](https://www.kaggle.com/code/dansbecker/stochastic-gradient-descent))  
- [ ] 04 - ê³¼ì í•©ê³¼ ê³¼ì†Œì í•© ([Overfitting and Underfitting](https://www.kaggle.com/code/dansbecker/overfitting-and-underfitting))  
- [ ] 05 - ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™” ([Dropout and Batch Normalization](https://www.kaggle.com/code/dansbecker/dropout-and-batch-normalization))  
- [ ] 06 - ì´ì§„ ë¶„ë¥˜ ([Binary Classification](https://www.kaggle.com/code/dansbecker/binary-classification))  

### Exercises
- [ ] 01 - ë‹¨ì¼ ë‰´ëŸ° ([A Single Neuron]())  
- [ ] 02 - ì‹¬ì¸µ ì‹ ê²½ë§ ([Deep Neural Networks]())  
- [ ] 03 - í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• ([Stochastic Gradient Descent]())  
- [ ] 04 - ê³¼ì í•©ê³¼ ê³¼ì†Œì í•© ([Overfitting and Underfitting]())  
- [ ] 05 - ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™” ([Dropout and Batch Normalization]())  
- [ ] 06 - ì´ì§„ ë¶„ë¥˜ ([Binary Classification]())  

