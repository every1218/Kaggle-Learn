# Kaggle-Learn

[![Kaggle](https://img.shields.io/badge/Kaggle-Learn-blue)](https://www.kaggle.com/learn)
[![Made with Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Kaggle 튜토리얼에서 공부한 내용과 실습 노트북을 기록, 정리하는 레포지토리입니다.

---

## 🐍 [Tutorial 1 - Python](https://www.kaggle.com/learn/python)  
⏳ 학습 기간: **25.06.24 ~ 25.07.13**

### Lecture Notes
- [x] 01 - 헬로 파이썬 ([Hello Python](https://www.kaggle.com/code/colinmorris/hello-python))
- [x] 02 - 함수와 도움말 ([Functions and Getting Help](https://www.kaggle.com/code/colinmorris/functions-and-getting-help))
- [x] 03 - 불리언과 조건문 ([Booleans and Conditionals](https://www.kaggle.com/code/colinmorris/booleans-and-conditionals))
- [x] 04 - 리스트 ([Lists](https://www.kaggle.com/code/colinmorris/lists))
- [x] 05 - 반복문과 리스트 컴프리헨션 ([Loops and List Comprehensions](https://www.kaggle.com/code/colinmorris/loops-and-list-comprehensions))
- [x] 06 - 문자열과 딕셔너리 ([Strings and Dictionaries](https://www.kaggle.com/code/colinmorris/strings-and-dictionaries))

### Exercises
- [x] 01 - 문법, 변수, 숫자 ([Syntax Variables and Numbers](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.06.24%20Hello%2C%20Python/exercise-syntax-variables-and-numbers.ipynb))  
- [x] 02 - 함수와 도움말 ([Functions and Getting Help](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.06.28%20Functions%20and%20Getting%20Help/exercise-functions-and-getting-help.ipynb))  
- [x] 03 - 불리언과 조건문 ([Booleans and Conditionals](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.07%20Booleans%20and%20Conditionals/exercise-booleans-and-conditionals.ipynb))  
- [x] 04 - 리스트 ([Lists](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.07%20Lists/exercise-lists.ipynb))  
- [x] 05 - 반복문과 리스트 컴프리헨션 ([Loops and List Comprehensions](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.10%20Loops%20and%20List%20Comprehensions/exercise-loops-and-list-comprehensions.ipynb))  
- [x] 06 - 문자열과 딕셔너리 ([Strings and Dictionaries](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.11%20Strings%20and%20Dictionaries/exercise-strings-and-dictionaries.ipynb))  
- [x] 07 - 외부 라이브러리 활용 ([Working with external Libraries](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.13%20Working%20with%20External%20Libraries/exercise-working-with-external-libraries.ipynb))

---

## 🐼 [Tutorial 2 - Pandas](https://www.kaggle.com/learn/pandas)  
⏳ 학습 기간: **25.06.29 ~ 25.07.17**

### Lecture Notes
- [x] 01 - 생성, 읽기, 쓰기 ([Creating, Reading, and Writing](https://www.kaggle.com/code/residentmario/creating-reading-and-writing))  
- [x] 02 - 인덱싱, 선택, 할당 ([Indexing, Selecting, and Assigning](https://www.kaggle.com/code/residentmario/indexing-selecting-assigning))  
- [x] 03 - 요약 함수와 매핑 ([Summary Functions and Maps](https://www.kaggle.com/code/residentmario/summary-functions-and-maps))  
- [x] 04 - 그룹화와 정렬 ([Grouping and Sorting](https://www.kaggle.com/code/residentmario/grouping-and-sorting))  
- [x] 05 - 데이터 타입과 결측치 ([Data Types and Missing Values](https://www.kaggle.com/code/residentmario/data-types-and-missing-values))  
- [x] 06 - 이름 변경과 결합 ([Renaming and Combining](https://www.kaggle.com/code/residentmario/renaming-and-combining))  

### Exercises
- [x] 01 - 생성, 읽기, 쓰기 ([Creating, Reading, and Writing]())  
- [x] 02 - 인덱싱, 선택, 할당 ([Indexing, Selecting, and Assigning]())  
- [x] 03 - 요약 함수와 매핑 ([Summary Functions and Maps]())  
- [x] 04 - 그룹화와 정렬 ([Grouping and Sorting]())  
- [x] 05 - 데이터 타입과 결측치 ([Data Types and Missing Values]())  
- [x] 06 - 이름 변경과 결합 ([Renaming and Combining]())  

---

## 🧹 [Tutorial 3 - Data Cleaning](https://www.kaggle.com/learn/data-cleaning)  
⏳ 학습 기간: **25.07.02 ~ 25.07.25**

### Lecture Notes
- [x] 01 - 결측치 다루기 ([Handling Missing Values](https://www.kaggle.com/code/alexisbcook/handling-missing-values))  
- [x] 02 - 스케일링과 정규화 ([Scaling and Normalization](https://www.kaggle.com/code/alexisbcook/scaling-and-normalization))  
- [x] 03 - 날짜 파싱 ([Parsing Dates](https://www.kaggle.com/code/alexisbcook/parsing-dates))  
- [x] 04 - 문자 인코딩 ([Character Encodings](https://www.kaggle.com/code/alexisbcook/character-encodings))  
- [x] 05 - 불일치 데이터 입력 ([Inconsistent Data Entry](https://www.kaggle.com/code/alexisbcook/inconsistent-data-entry))  

### Exercises
- [x] 01 - 결측치 다루기 ([Handling Missing Values]())  
- [x] 02 - 스케일링과 정규화 ([Scaling and Normalization]())  
- [x] 03 - 날짜 파싱 ([Parsing Dates]())  
- [x] 04 - 문자 인코딩 ([Character Encodings]())  
- [x] 05 - 불일치 데이터 입력 ([Inconsistent Data Entry]())  

---

## 🤖 [Tutorial 4 - Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)  
⏳ 학습 기간: **25.07.06 ~ 25.08.08**

### Lecture Notes
- [x] 01 - 모델의 작동 원리 ([How Models Work](https://www.kaggle.com/code/dansbecker/how-models-work))  
- [x] 02 - 기본 데이터 탐색 ([Basic Data Exploration](https://www.kaggle.com/code/dansbecker/basic-data-exploration))  
- [x] 03 - 첫 머신러닝 모델 ([Your First Machine Learning Model](https://www.kaggle.com/code/dansbecker/your-first-machine-learning-model))  
- [x] 04 - 모델 검증 ([Model Validation](https://www.kaggle.com/code/dansbecker/model-validation))  
- [x] 05 - 과소적합과 과적합 ([Underfitting and Overfitting](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting))  
- [x] 06 - 랜덤 포레스트 ([Random Forests](https://www.kaggle.com/code/dansbecker/random-forests))  
- [x] 07 - 머신러닝 대회 ([Machine Learning Competitions](https://www.kaggle.com/code/dansbecker/machine-learning-competitions))  

### Exercises
- [x] 02 - 데이터 탐색 ([Explore your data]())  
- [x] 03 - 첫 머신러닝 모델 ([Your First Machine Learning Model]())  
- [x] 04 - 모델 검증 ([Model Validation]())  
- [x] 05 - 과소적합과 과적합 ([Underfitting and Overfitting]())  
- [x] 06 - 랜덤 포레스트 ([Random Forests]())  
- [x] 07 - 머신러닝 대회 ([Machine Learning Competitions]())  

---

## 📈 [Tutorial 5 - Data Visualization](https://www.kaggle.com/learn/data-visualization)  
⏳ 학습 기간: **25.07.01 ~ 25.08.16**

### Lecture Notes
- [x] 01 - 헬로 시본 ([Hello Seaborn](https://www.kaggle.com/code/alexisbcook/hello-seaborn))  
- [x] 02 - 선 그래프 ([Line Charts](https://www.kaggle.com/code/alexisbcook/line-charts))  
- [x] 03 - 막대 그래프와 히트맵 ([Bar Charts and Heatmaps](https://www.kaggle.com/code/alexisbcook/bar-charts-and-heatmaps))  
- [x] 04 - 산점도 ([Scatter Plots](https://www.kaggle.com/code/alexisbcook/scatter-plots))  
- [x] 05 - 분포 ([Distributions](https://www.kaggle.com/code/alexisbcook/distributions))  
- [x] 06 - 그래프 선택과 스타일 ([Choosing Plot Types and Custom Styles](https://www.kaggle.com/code/alexisbcook/choosing-plot-types-and-custom-styles))  
- [x] 07 - 최종 프로젝트 ([Final Project](https://www.kaggle.com/code/alexisbcook/final-project))  
- [x] 08 - 나만의 노트북 만들기 ([Creating Your Own Notebook](https://www.kaggle.com/code/alexisbcook/creating-your-own-notebook))  

### Exercises
- [x] 01 - 헬로 시본 ([Hello Seaborn]())  
- [x] 02 - 선 그래프 ([Line Charts]())  
- [x] 03 - 막대 그래프와 히트맵 ([Bar Charts and Heatmaps]())  
- [x] 04 - 산점도 ([Scatter Plots]())  
- [x] 05 - 분포 ([Distributions]())  
- [x] 06 - 그래프 선택과 스타일 ([Choosing Plot Types and Custom Styles]())  
- [x] 07 - 최종 프로젝트 ([Final Project]())  

---

## 📊 [Tutorial 6 - Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)  
⏳ 학습 기간: 

### Lecture Notes
- [ ] 01 - 소개 ([Introduction](https://www.kaggle.com/code/alexisbcook/introduction))  
- [ ] 02 - 결측치 ([Missing Values](https://www.kaggle.com/code/alexisbcook/missing-values))  
- [ ] 03 - 범주형 변수 ([Categorical Variables](https://www.kaggle.com/code/alexisbcook/categorical-variables))  
- [ ] 04 - 파이프라인 ([Pipelines](https://www.kaggle.com/code/alexisbcook/pipelines))  
- [ ] 05 - 교차 검증 ([Cross-Validation](https://www.kaggle.com/code/alexisbcook/cross-validation))  
- [ ] 06 - XGBoost ([XGBoost](https://www.kaggle.com/code/alexisbcook/xgboost))  
- [ ] 07 - 데이터 누수 ([Data Leakage](https://www.kaggle.com/code/alexisbcook/data-leakage))  

### Exercises
- [ ] 01 - 소개 ([Introduction]())  
- [ ] 02 - 결측치 ([Missing Values]())  
- [ ] 03 - 범주형 변수 ([Categorical Variables]())  
- [ ] 04 - 파이프라인 ([Pipelines]())  
- [ ] 05 - 교차 검증 ([Cross-Validation]())  
- [ ] 06 - XGBoost ([XGBoost]())  
- [ ] 07 - 데이터 누수 ([Data Leakage]())

---

## 🧠 [Tutorial 7 - Introduction to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)  
⏳ 학습 기간: 

### Lecture Notes
- [ ] 01 - 단일 뉴런 ([A Single Neuron](https://www.kaggle.com/code/dansbecker/a-single-neuron))  
- [ ] 02 - 심층 신경망 ([Deep Neural Networks](https://www.kaggle.com/code/dansbecker/deep-neural-networks))  
- [ ] 03 - 확률적 경사 하강법 ([Stochastic Gradient Descent](https://www.kaggle.com/code/dansbecker/stochastic-gradient-descent))  
- [ ] 04 - 과적합과 과소적합 ([Overfitting and Underfitting](https://www.kaggle.com/code/dansbecker/overfitting-and-underfitting))  
- [ ] 05 - 드롭아웃과 배치 정규화 ([Dropout and Batch Normalization](https://www.kaggle.com/code/dansbecker/dropout-and-batch-normalization))  
- [ ] 06 - 이진 분류 ([Binary Classification](https://www.kaggle.com/code/dansbecker/binary-classification))  

### Exercises
- [ ] 01 - 단일 뉴런 ([A Single Neuron]())  
- [ ] 02 - 심층 신경망 ([Deep Neural Networks]())  
- [ ] 03 - 확률적 경사 하강법 ([Stochastic Gradient Descent]())  
- [ ] 04 - 과적합과 과소적합 ([Overfitting and Underfitting]())  
- [ ] 05 - 드롭아웃과 배치 정규화 ([Dropout and Batch Normalization]())  
- [ ] 06 - 이진 분류 ([Binary Classification]())  

