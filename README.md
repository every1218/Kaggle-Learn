# Kaggle-Learn

[![Kaggle](https://img.shields.io/badge/Kaggle-Learn-blue)](https://www.kaggle.com/learn)
[![Made with Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Kaggle 튜토리얼에서 공부한 내용과 실습 노트북을 기록, 정리하는 레포지토리입니다.

완료(✅), 진행 중(🔥), 예정(📅)

---

## 🐍 [01 - Python](https://www.kaggle.com/learn/python)  
[✅ **완료**]

⏳ 학습 기간: **25.06.24 ~ 25.07.13**

### Lecture Notes
- [x] 01 - 헬로 파이썬 ([Hello Python](https://www.kaggle.com/code/colinmorris/hello-python))
- [x] 02 - 함수와 도움말 ([Functions and Getting Help](https://www.kaggle.com/code/colinmorris/functions-and-getting-help))
- [x] 03 - 불리언과 조건문 ([Booleans and Conditionals](https://www.kaggle.com/code/colinmorris/booleans-and-conditionals))
- [x] 04 - 리스트 ([Lists](https://www.kaggle.com/code/colinmorris/lists))
- [x] 05 - 반복문과 리스트 컴프리헨션 ([Loops and List Comprehensions](https://www.kaggle.com/code/colinmorris/loops-and-list-comprehensions))
- [x] 06 - 문자열과 딕셔너리 ([Strings and Dictionaries](https://www.kaggle.com/code/colinmorris/strings-and-dictionaries))

### Exercises
- [x] 01 - 문법, 변수, 숫자 ([Syntax Variables and Numbers](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.06.24%20Hello%2C%20Python/exercise-syntax-variables-and-numbers.ipynb))  
- [x] 02 - 함수와 도움말 ([Functions and Getting Help](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.06.28%20Functions%20and%20Getting%20Help/exercise-functions-and-getting-help.ipynb))  
- [x] 03 - 불리언과 조건문 ([Booleans and Conditionals](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.07%20Booleans%20and%20Conditionals/exercise-booleans-and-conditionals.ipynb))  
- [x] 04 - 리스트 ([Lists](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.07%20Lists/exercise-lists.ipynb))  
- [x] 05 - 반복문과 리스트 컴프리헨션 ([Loops and List Comprehensions](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.10%20Loops%20and%20List%20Comprehensions/exercise-loops-and-list-comprehensions.ipynb))  
- [x] 06 - 문자열과 딕셔너리 ([Strings and Dictionaries](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.11%20Strings%20and%20Dictionaries/exercise-strings-and-dictionaries.ipynb))  
- [x] 07 - 외부 라이브러리 활용 ([Working with external Libraries](https://github.com/every1218/Kaggle-Learn/blob/main/Python/25.07.13%20Working%20with%20External%20Libraries/exercise-working-with-external-libraries.ipynb))

---

## 🐼 [02 - Pandas](https://www.kaggle.com/learn/pandas)  
[✅ **완료**]

⏳ 학습 기간: **25.06.29 ~ 25.07.17**

### Lecture Notes
- [x] 01 - 생성, 읽기, 쓰기 ([Creating, Reading, and Writing](https://www.kaggle.com/code/residentmario/creating-reading-and-writing))  
- [x] 02 - 인덱싱, 선택, 할당 ([Indexing, Selecting, and Assigning](https://www.kaggle.com/code/residentmario/indexing-selecting-assigning))  
- [x] 03 - 요약 함수와 매핑 ([Summary Functions and Maps](https://www.kaggle.com/code/residentmario/summary-functions-and-maps))  
- [x] 04 - 그룹화와 정렬 ([Grouping and Sorting](https://www.kaggle.com/code/residentmario/grouping-and-sorting))  
- [x] 05 - 데이터 타입과 결측치 ([Data Types and Missing Values](https://www.kaggle.com/code/residentmario/data-types-and-missing-values))  
- [x] 06 - 이름 변경과 결합 ([Renaming and Combining](https://www.kaggle.com/code/residentmario/renaming-and-combining))  

### Exercises
- [x] 01 - 생성, 읽기, 쓰기 ([Creating, Reading, and Writing](https://github.com/every1218/Kaggle-Learn/blob/main/Pandas/25.06.29%20Creating%2C%20Reading%20and%20Writing/exercise-creating-reading-and-writing.ipynb))  
- [x] 02 - 인덱싱, 선택, 할당 ([Indexing, Selecting, and Assigning](https://github.com/every1218/Kaggle-Learn/blob/main/Pandas/25.07.03%20Indexing%2C%20Selecting%20%26%20Assigning/exercise-indexing-selecting-assigning.ipynb))  
- [x] 03 - 요약 함수와 매핑 ([Summary Functions and Maps](https://github.com/every1218/Kaggle-Learn/blob/main/Pandas/25.07.07%20Summary%20Functions%20and%20Maps/exercise-summary-functions-and-maps.ipynb))  
- [x] 04 - 그룹화와 정렬 ([Grouping and Sorting](https://github.com/every1218/Kaggle-Learn/blob/main/Pandas/25.07.14%20Grouping%20and%20Sorting/exercise-grouping-and-sorting.ipynb))  
- [x] 05 - 데이터 타입과 결측치 ([Data Types and Missing Values](https://github.com/every1218/Kaggle-Learn/blob/main/Pandas/25.07.17%20Data%20Types%20and%20Missing%20Values/exercise-data-types-and-missing-values.ipynb))  
- [x] 06 - 이름 변경과 결합 ([Renaming and Combining](https://github.com/every1218/Kaggle-Learn/blob/main/Pandas/25.07.17%20Renaming%20and%20Combining/exercise-renaming-and-combining.ipynb))  

---

## 🧹 [03 - Data Cleaning](https://www.kaggle.com/learn/data-cleaning)  
[✅ **완료**]

⏳ 학습 기간: **25.07.02 ~ 25.07.25**

### Lecture Notes
- [x] 01 - 결측치 다루기 ([Handling Missing Values](https://www.kaggle.com/code/alexisbcook/handling-missing-values))  
- [x] 02 - 스케일링과 정규화 ([Scaling and Normalization](https://www.kaggle.com/code/alexisbcook/scaling-and-normalization)) 
- [x] 03 - 날짜 파싱 ([Parsing Dates](https://www.kaggle.com/code/alexisbcook/parsing-dates))  
- [x] 04 - 문자 인코딩 ([Character Encodings](https://www.kaggle.com/code/alexisbcook/character-encodings))  
- [x] 05 - 불일치 데이터 입력 ([Inconsistent Data Entry](https://www.kaggle.com/code/alexisbcook/inconsistent-data-entry))  

### Exercises
- [x] 01 - 결측치 다루기 ([Handling Missing Values](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Cleaning/25.07.02%20Handling%20Missing%20Values/exercise-handling-missing-values.ipynb))  
- [x] 02 - 스케일링과 정규화 ([Scaling and Normalization](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Cleaning/25.07.06%20Scaling%20and%20Normalization/exercise-scaling-and-normalization.ipynb))  
- [x] 03 - 날짜 파싱 ([Parsing Dates](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Cleaning/25.07.12%20Parsing%20Dates/exercise-parsing-dates.ipynb))  
- [x] 04 - 문자 인코딩 ([Character Encodings](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Cleaning/25.07.25%20Character%20Encodings/exercise-character-encodings.ipynb))  
- [x] 05 - 불일치 데이터 입력 ([Inconsistent Data Entry](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Cleaning/25.07.25%20Inconsistent%20Data%20Entry/exercise-inconsistent-data-entry.ipynb))  

---

## 🤖 [04 - Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)  
[✅ **완료**]

⏳ 학습 기간: **25.07.06 ~ 25.08.08**

### Lecture Notes
- [x] 01 - 모델의 작동 원리 ([How Models Work](https://www.kaggle.com/code/dansbecker/how-models-work))  
- [x] 02 - 기본 데이터 탐색 ([Basic Data Exploration](https://www.kaggle.com/code/dansbecker/basic-data-exploration))  
- [x] 03 - 첫 머신러닝 모델 ([Your First Machine Learning Model](https://www.kaggle.com/code/dansbecker/your-first-machine-learning-model))  
- [x] 04 - 모델 검증 ([Model Validation](https://www.kaggle.com/code/dansbecker/model-validation))  
- [x] 05 - 과소적합과 과적합 ([Underfitting and Overfitting](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting))  
- [x] 06 - 랜덤 포레스트 ([Random Forests](https://www.kaggle.com/code/dansbecker/random-forests))  
- [x] 07 - 머신러닝 대회 ([Machine Learning Competitions](https://www.kaggle.com/code/dansbecker/machine-learning-competitions))  

### Exercises
- [x] 02 - 데이터 탐색 ([Explore your data](https://github.com/every1218/Kaggle-Learn/blob/main/Intro%20to%20Machine%20Learning/25.07.06%20Basic%20Data%20Exploration/exercise-explore-your-data.ipynb))  
- [x] 03 - 첫 머신러닝 모델 ([Your First Machine Learning Model](https://github.com/every1218/Kaggle-Learn/blob/main/Intro%20to%20Machine%20Learning/25.07.06%20Your%20First%20Machine%20Learning%20Model/exercise-your-first-machine-learning-model.ipynb))  
- [x] 04 - 모델 검증 ([Model Validation](https://github.com/every1218/Kaggle-Learn/blob/main/Intro%20to%20Machine%20Learning/25.07.07%20Model%20Validation/exercise-model-validation.ipynb))  
- [x] 05 - 과소적합과 과적합 ([Underfitting and Overfitting](https://github.com/every1218/Kaggle-Learn/blob/main/Intro%20to%20Machine%20Learning/25.07.21%20Underfitting%20and%20Overfitting/exercise-underfitting-and-overfitting.ipynb))  
- [x] 06 - 랜덤 포레스트 ([Random Forests](https://github.com/every1218/Kaggle-Learn/blob/main/Intro%20to%20Machine%20Learning/25.08.08%20Random%20Forests/exercise-random-forests.ipynb))  
- [x] 07 - 머신러닝 대회 ([Machine Learning Competitions](https://github.com/every1218/Kaggle-Learn/blob/main/Intro%20to%20Machine%20Learning/25.08.08%20Machine%20Learning%20Competitions/exercise-machine-learning-competitions.ipynb))  

---

## 📈 [05 - Data Visualization](https://www.kaggle.com/learn/data-visualization)  
[✅ **완료**]

⏳ 학습 기간: **25.07.01 ~ 25.08.16**

### Lecture Notes
- [x] 01 - 헬로 시본 ([Hello Seaborn](https://www.kaggle.com/code/alexisbcook/hello-seaborn))  
- [x] 02 - 선 그래프 ([Line Charts](https://www.kaggle.com/code/alexisbcook/line-charts))  
- [x] 03 - 막대 그래프와 히트맵 ([Bar Charts and Heatmaps](https://www.kaggle.com/code/alexisbcook/bar-charts-and-heatmaps))  
- [x] 04 - 산점도 ([Scatter Plots](https://www.kaggle.com/code/alexisbcook/scatter-plots))  
- [x] 05 - 분포 ([Distributions](https://www.kaggle.com/code/alexisbcook/distributions))  
- [x] 06 - 그래프 선택과 스타일 ([Choosing Plot Types and Custom Styles](https://www.kaggle.com/code/alexisbcook/choosing-plot-types-and-custom-styles))  
- [x] 07 - 최종 프로젝트 ([Final Project](https://www.kaggle.com/code/alexisbcook/final-project))  
- [x] 08 - 나만의 노트북 만들기 ([Creating Your Own Notebook](https://www.kaggle.com/code/alexisbcook/creating-your-own-notebook))  

### Exercises
- [x] 01 - 헬로 시본 ([Hello Seaborn](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.07.01%20Hello%2C%20Seaborn/exercise-hello-seaborn.ipynb))  
- [x] 02 - 선 그래프 ([Line Charts](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.07.01%20Line%20Charts/exercise-line-charts.ipynb))  
- [x] 03 - 막대 그래프와 히트맵 ([Bar Charts and Heatmaps](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.07.05%20Bar%20Charts%20and%20Heatmaps/exercise-bar-charts-and-heatmaps.ipynb))  
- [x] 04 - 산점도 ([Scatter Plots](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.07.16%20Scatter%20Plots/exercise-scatter-plots.ipynb))  
- [x] 05 - 분포 ([Distributions](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.08.09%20Distributions/exercise-distributions.ipynb))  
- [x] 06 - 그래프 선택과 스타일 ([Choosing Plot Types and Custom Styles](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.08.16%20Choosing%20Plot%20Types%20and%20Custom%20Styles/exercise-choosing-plot-types-and-custom-styles.ipynb))  
- [x] 07 - 최종 프로젝트 ([Final Project](https://github.com/every1218/Kaggle-Learn/blob/main/Data%20Visualization/25.08.16%20Final%20Project/exercise-final-project.ipynb))  

---

## 📊 [06 - Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)  
[🔥 **진행 중**]

⏳ 학습 기간: 

### Lecture Notes
- [ ] 01 - 소개 ([Introduction](https://www.kaggle.com/code/alexisbcook/introduction))  
- [ ] 02 - 결측치 ([Missing Values](https://www.kaggle.com/code/alexisbcook/missing-values))  
- [ ] 03 - 범주형 변수 ([Categorical Variables](https://www.kaggle.com/code/alexisbcook/categorical-variables))  
- [ ] 04 - 파이프라인 ([Pipelines](https://www.kaggle.com/code/alexisbcook/pipelines))  
- [ ] 05 - 교차 검증 ([Cross-Validation](https://www.kaggle.com/code/alexisbcook/cross-validation))  
- [ ] 06 - XGBoost ([XGBoost](https://www.kaggle.com/code/alexisbcook/xgboost))  
- [ ] 07 - 데이터 누수 ([Data Leakage](https://www.kaggle.com/code/alexisbcook/data-leakage))  

### Exercises
- [ ] 01 - 소개 ([Introduction]())  
- [ ] 02 - 결측치 ([Missing Values]())  
- [ ] 03 - 범주형 변수 ([Categorical Variables]())  
- [ ] 04 - 파이프라인 ([Pipelines]())  
- [ ] 05 - 교차 검증 ([Cross-Validation]())  
- [ ] 06 - XGBoost ([XGBoost]())  
- [ ] 07 - 데이터 누수 ([Data Leakage]())

---

## 🧠 [07 - Introduction to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)  
[📅 **예정**]

⏳ 학습 기간: 

### Lecture Notes
- [ ] 01 - 단일 뉴런 ([A Single Neuron](https://www.kaggle.com/code/dansbecker/a-single-neuron))  
- [ ] 02 - 심층 신경망 ([Deep Neural Networks](https://www.kaggle.com/code/dansbecker/deep-neural-networks))  
- [ ] 03 - 확률적 경사 하강법 ([Stochastic Gradient Descent](https://www.kaggle.com/code/dansbecker/stochastic-gradient-descent))  
- [ ] 04 - 과적합과 과소적합 ([Overfitting and Underfitting](https://www.kaggle.com/code/dansbecker/overfitting-and-underfitting))  
- [ ] 05 - 드롭아웃과 배치 정규화 ([Dropout and Batch Normalization](https://www.kaggle.com/code/dansbecker/dropout-and-batch-normalization))  
- [ ] 06 - 이진 분류 ([Binary Classification](https://www.kaggle.com/code/dansbecker/binary-classification))  

### Exercises
- [ ] 01 - 단일 뉴런 ([A Single Neuron]())  
- [ ] 02 - 심층 신경망 ([Deep Neural Networks]())  
- [ ] 03 - 확률적 경사 하강법 ([Stochastic Gradient Descent]())  
- [ ] 04 - 과적합과 과소적합 ([Overfitting and Underfitting]())  
- [ ] 05 - 드롭아웃과 배치 정규화 ([Dropout and Batch Normalization]())  
- [ ] 06 - 이진 분류 ([Binary Classification]())  

